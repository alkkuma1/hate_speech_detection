{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Davidson Dataset Hate Speech Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alkakumari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alkakumari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12681.192027</td>\n",
       "      <td>3.243473</td>\n",
       "      <td>0.280515</td>\n",
       "      <td>2.413711</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>1.110277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7299.553863</td>\n",
       "      <td>0.883060</td>\n",
       "      <td>0.631851</td>\n",
       "      <td>1.399459</td>\n",
       "      <td>1.113299</td>\n",
       "      <td>0.462089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6372.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12703.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18995.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25296.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0         count   hate_speech  offensive_language  \\\n",
       "count  24783.000000  24783.000000  24783.000000        24783.000000   \n",
       "mean   12681.192027      3.243473      0.280515            2.413711   \n",
       "std     7299.553863      0.883060      0.631851            1.399459   \n",
       "min        0.000000      3.000000      0.000000            0.000000   \n",
       "25%     6372.500000      3.000000      0.000000            2.000000   \n",
       "50%    12703.000000      3.000000      0.000000            3.000000   \n",
       "75%    18995.500000      3.000000      0.000000            3.000000   \n",
       "max    25296.000000      9.000000      7.000000            9.000000   \n",
       "\n",
       "            neither         class  \n",
       "count  24783.000000  24783.000000  \n",
       "mean       0.549247      1.110277  \n",
       "std        1.113299      0.462089  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      1.000000  \n",
       "50%        0.000000      1.000000  \n",
       "75%        0.000000      1.000000  \n",
       "max        9.000000      2.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "david_df = pd.read_csv('data/davidson.csv')\n",
    "david_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data definition:\n",
    "\n",
    "count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n",
    "\n",
    "hate_speech = number of CF users who judged the tweet to be hate speech.\n",
    "\n",
    "offensive_language = number of CF users who judged the tweet to be offensive.\n",
    "\n",
    "neither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n",
    "\n",
    "class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            0\n",
       "count                 0\n",
       "hate_speech           0\n",
       "offensive_language    0\n",
       "neither               0\n",
       "class                 0\n",
       "tweet                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "david_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('&amp;') # &amp; means and\n",
    "def clean(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: x.lower())\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'@[A-Za-z0-9]*', 'MENTION', x))\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', 'URL', x))\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join([wnl.lemmatize(word, pos='a') for word in x.split()])) #adjectives\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join([wnl.lemmatize(word, pos='v') for word in x.split()])) #verbs\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join([wnl.lemmatize(word, pos='n') for word in x.split()])) #noun\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(df['tweet'])\n",
    "    df['tweet'] = tokenizer.texts_to_sequences(df['tweet'])\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return df, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = clean(df)\n",
    "    df, vocab_size = tokenize(df)\n",
    "    return df, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[3, 1, 110, 620, 550, 228, 39, 92, 43, 18, 347]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 100, 79, 8829, 6079, 33, 2146, 79, 4, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 683, 3, 1, 111, 8, 2, 125, 259, 790, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 602, 4778, 1, 3411, 24, 7, 592]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 15, 208, 254, 448, 254, 4779, 2, 41, 47...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0    [3, 1, 110, 620, 550, 228, 39, 92, 43, 18, 347]  \n",
       "1  [3, 1, 100, 79, 8829, 6079, 33, 2146, 79, 4, 7...  \n",
       "2    [3, 1, 683, 3, 1, 111, 8, 2, 125, 259, 790, 15]  \n",
       "3             [3, 1, 602, 4778, 1, 3411, 24, 7, 592]  \n",
       "4  [3, 1, 15, 208, 254, 448, 254, 4779, 2, 41, 47...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df, vocab_size = preprocess(david_df)\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21685, 6) (4957, 6) (3098, 6)\n"
     ]
    }
   ],
   "source": [
    "X = preprocessed_df.drop(columns=['class'])\n",
    "Y = preprocessed_df['class']\n",
    "#X_train_text, X_test_text, X_train_add, X_test_add, y_train, y_test = train_test_split(\n",
    "    #tweet_padded, additional_features_scaled, df['class'], test_size=0.2, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=54)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.125, random_state=54) # 0.125 x 0.8 = 0.1\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded_tweet = tf.keras.preprocessing.sequence.pad_sequences(X_train['tweet'].tolist(), padding='post', maxlen=50)\n",
    "val_padded_tweet = tf.keras.preprocessing.sequence.pad_sequences(X_val['tweet'].tolist(), padding='post', maxlen=50)\n",
    "test_padded_tweet = tf.keras.preprocessing.sequence.pad_sequences(X_test['tweet'].tolist(), padding='post', maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21685, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_add_features = X_train[['count', 'hate_speech', 'offensive_language', 'neither']].values\n",
    "val_add_features = X_val[['count', 'hate_speech', 'offensive_language', 'neither']].values\n",
    "test_add_features = X_test[['count', 'hate_speech', 'offensive_language', 'neither']].values\n",
    "\n",
    "train_add_features_scaled = scaler.fit(train_add_features).transform(train_add_features)\n",
    "val_add_features_scaled = scaler.fit(val_add_features).transform(val_add_features)\n",
    "test_add_features_scaled = scaler.fit(test_add_features).transform(test_add_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.91516639 0.42954203 1.98853737]\n"
     ]
    }
   ],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(Y_train), y=Y_train)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23565\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_padded_tweet), type(train_add_features_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSTM(vocab_size, input_length=50):\n",
    "    learning_rate = 0.0001\n",
    "    Input = tf.keras.layers.Input\n",
    "    Embedding = tf.keras.layers.Embedding\n",
    "    LSTM = tf.keras.layers.LSTM\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    Dropout = tf.keras.layers.Dropout\n",
    "    Concatenate = tf.keras.layers.Concatenate\n",
    "    Model = tf.keras.Model\n",
    "\n",
    "    text_input = Input(shape=(input_length,), name='text_input')\n",
    "    x = Embedding(input_dim=vocab_size+1, output_dim=256, input_length=input_length)(text_input)\n",
    "    x = LSTM(128)(x)\n",
    "    add_input = Input(shape=(4,), name='additional_input')\n",
    "    x = Concatenate()([x, add_input])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(1, activation='softmax')(x)\n",
    "    model = Model(inputs=[text_input, add_input], outputs=output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_logarithmic_error', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_RNN(vocab_size, input_length=50):\n",
    "    learning_rate = 0.0001\n",
    "    Input = tf.keras.layers.Input\n",
    "    Embedding = tf.keras.layers.Embedding\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    Dropout = tf.keras.layers.Dropout\n",
    "    Concatenate = tf.keras.layers.Concatenate\n",
    "    Model = tf.keras.Model\n",
    "    simpleRNN = tf.keras.layers.SimpleRNN\n",
    "\n",
    "    text_input = Input(shape=(input_length,), name='text_input')\n",
    "    x = Embedding(input_dim=vocab_size+1, output_dim=256, input_length=input_length)(text_input)\n",
    "    x = simpleRNN(128)(x)\n",
    "    add_input = Input(shape=(4,), name='additional_input')\n",
    "    x = Concatenate()([x, add_input])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(1, activation='softmax')(x)\n",
    "    model = Model(inputs=[text_input, add_input], outputs=output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_logarithmic_error', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_GRU(vocab_size, input_length=50):\n",
    "    learning_rate = 0.0001\n",
    "    Input = tf.keras.layers.Input\n",
    "    Embedding = tf.keras.layers.Embedding\n",
    "    GRU = tf.keras.layers.GRU\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    Dropout = tf.keras.layers.Dropout\n",
    "    Concatenate = tf.keras.layers.Concatenate\n",
    "    Model = tf.keras.Model\n",
    "\n",
    "    text_input = Input(shape=(input_length,), name='text_input')\n",
    "    x = Embedding(input_dim=vocab_size+1, output_dim=256, input_length=input_length)(text_input)\n",
    "    x = GRU(128)(x)\n",
    "    add_input = Input(shape=(4,), name='additional_input')\n",
    "    x = Concatenate()([x, add_input])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(1, activation='softmax')(x)\n",
    "    model = Model(inputs=[text_input, add_input], outputs=output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_logarithmic_error', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, Y_train, X_val, Y_val, epochs=25, batch_size=128):\n",
    "    model.fit(X_train, Y_train, \n",
    "          validation_data=(X_val, Y_val), \n",
    "          epochs=epochs, batch_size=64, \n",
    "          verbose=1)\n",
    "    print(\"\\n\\n****************************\\n\\n\")\n",
    "    print(\"Model trained successfully\")\n",
    "    pred = model.predict(X_val, batch_size=batch_size, verbose=1, steps=None)\n",
    "    print(\"Predictions: \", np.round(pred, decimals=2))\n",
    "    print(\"Validation Accuracy: \", model.evaluate(X_val, Y_val, batch_size=batch_size, verbose=1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 76ms/step - accuracy: 0.7726 - loss: 0.0562 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 2/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 75ms/step - accuracy: 0.7779 - loss: 0.0542 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 3/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 77ms/step - accuracy: 0.7804 - loss: 0.0537 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 4/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 74ms/step - accuracy: 0.7740 - loss: 0.0550 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 5/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 80ms/step - accuracy: 0.7729 - loss: 0.0549 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "\n",
      "\n",
      "****************************\n",
      "\n",
      "\n",
      "Model trained successfully\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "Predictions:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.7653 - loss: 0.0587\n",
      "Validation Accuracy:  [0.05793875828385353, 0.7678031325340271]\n"
     ]
    }
   ],
   "source": [
    "X_train = [train_padded_tweet, train_add_features_scaled]\n",
    "X_val = [val_padded_tweet, val_add_features_scaled]\n",
    "m = model_LSTM(vocab_size, input_length=50)\n",
    "model = train(m, X_train, Y_train, X_val, Y_val, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - accuracy: 0.7789 - loss: 0.0536 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 2/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 45ms/step - accuracy: 0.7778 - loss: 0.0542 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 3/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - accuracy: 0.7759 - loss: 0.0548 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 4/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - accuracy: 0.7779 - loss: 0.0542 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 5/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 43ms/step - accuracy: 0.7774 - loss: 0.0542 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "\n",
      "\n",
      "****************************\n",
      "\n",
      "\n",
      "Model trained successfully\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Predictions:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7653 - loss: 0.0587\n",
      "Validation Accuracy:  [0.05793875828385353, 0.7678031325340271]\n"
     ]
    }
   ],
   "source": [
    "X_train = [train_padded_tweet, train_add_features_scaled]\n",
    "X_val = [val_padded_tweet, val_add_features_scaled]\n",
    "m = model_RNN(vocab_size, input_length=50)\n",
    "model = train(m, X_train, Y_train, X_val, Y_val, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 73ms/step - accuracy: 0.7771 - loss: 0.0548 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 2/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 74ms/step - accuracy: 0.7684 - loss: 0.0570 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 3/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 74ms/step - accuracy: 0.7721 - loss: 0.0547 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 4/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 78ms/step - accuracy: 0.7786 - loss: 0.0534 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "Epoch 5/5\n",
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 81ms/step - accuracy: 0.7781 - loss: 0.0537 - val_accuracy: 0.7678 - val_loss: 0.0579\n",
      "\n",
      "\n",
      "****************************\n",
      "\n",
      "\n",
      "Model trained successfully\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "Predictions:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7653 - loss: 0.0587\n",
      "Validation Accuracy:  [0.05793875828385353, 0.7678031325340271]\n"
     ]
    }
   ],
   "source": [
    "X_train = [train_padded_tweet, train_add_features_scaled]\n",
    "X_val = [val_padded_tweet, val_add_features_scaled]\n",
    "m = model_GRU(vocab_size, input_length=50)\n",
    "model = train(m, X_train, Y_train, X_val, Y_val, epochs=5, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
