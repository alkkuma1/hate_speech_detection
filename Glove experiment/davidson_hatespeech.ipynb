{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Davidson Dataset Hate Speech Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "Input = tf.keras.layers.Input\n",
    "Embedding = tf.keras.layers.Embedding\n",
    "Dense = tf.keras.layers.Dense\n",
    "Concatenate = tf.keras.layers.Concatenate\n",
    "Model = tf.keras.Model\n",
    "simpleRNN = tf.keras.layers.SimpleRNN\n",
    "LSTM = tf.keras.layers.LSTM\n",
    "GRU = tf.keras.layers.GRU\n",
    "dropout = tf.keras.layers.Dropout\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\"\"\" import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:y\n",
    "\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context \"\"\"\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data definition:\n",
    "\n",
    "count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n",
    "\n",
    "hate_speech = number of CF users who judged the tweet to be hate speech.\n",
    "\n",
    "offensive_language = number of CF users who judged the tweet to be offensive.\n",
    "\n",
    "neither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n",
    "\n",
    "class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "david_df = pd.read_csv('data/davidson.csv')\n",
    "david_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Preprocess definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "david_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('&amp;') # &amp; means and\n",
    "def clean(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: x.lower()) #lowercase\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)])) #stopwords removal\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'@[A-Za-z0-9]*', 'MENTION', x)) #replace all @mentions to 'MENTION'\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', 'URL', x)) #replace all urls to 'URL'\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)) #remove punctuation\n",
    "    punct = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_{|}~'\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punct)))\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join([wnl.lemmatize(word, pos='a') for word in x.split()])) #lemmatize on the basis of adjectives\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join([wnl.lemmatize(word, pos='v') for word in x.split()])) #lemmatize on the basis of verbs\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join([wnl.lemmatize(word, pos='n') for word in x.split()])) #lemmatize on the basis of noun\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'\\bRT\\b', '', x)) #remove RT\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 25\n",
    "def tokenize(df):\n",
    "    df['tweet'] = tokenizer.texts_to_sequences(df['tweet'])\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    #print('Vocabulary size: {}'.format(vocab_size))\n",
    "    padded_tweet = pad_sequences(df['tweet'].tolist(), padding='post', maxlen=MAX_SEQ_LEN)\n",
    "    return padded_tweet, vocab_size\n",
    "\n",
    "def preprocess(df):\n",
    "    df = clean(df)\n",
    "    df, vocab_size = tokenize(df)\n",
    "    return df, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset and calling preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenizer is fit on the training data only, which is used to transform both the training and test data to maintain the integrity of the val set as truly unseen data. This avoids data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = david_df.drop(columns=['class'])\n",
    "Y = david_df['class']\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=54)\n",
    "print(\"Training Shape:\", X_train.shape)\n",
    "print(\"Validation Shape:\", X_val.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after splitting the dataset, the distribution, although imbalanced, remains more or less consistent throughout original, train, val and test dataset. Hence, this splitting is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Class Distribution')\n",
    "plt.figure(figsize=(15, 5))\n",
    "explode = (0, 0, 0.1)\n",
    "plt.subplot(1, 3, 1)\n",
    "david_df['class_labels'] = david_df['class'].map({0: 'Hate', 1: 'Offensive', 2: 'Neither'})\n",
    "david_df['class_labels'].value_counts().plot(kind='pie', title='Original', colormap='Accent', \n",
    "                                      autopct='%1.1f%%',\n",
    "                                      explode=explode,\n",
    "                                      )\n",
    "plt.subplot(1, 3, 2)\n",
    "Y_train.value_counts().plot.pie(title='Training', colormap='Accent', autopct='%1.1f%%',\n",
    "                                explode=explode)\n",
    "plt.subplot(1, 3, 3)\n",
    "Y_val.value_counts().plot.pie(title='Validation', colormap='Accent', autopct='%1.1f%%',\n",
    "                                explode=explode)\n",
    "plt.show()\n",
    "david_df.drop(columns=['class_labels'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train['tweet'])\n",
    "X_train, vocab_size = preprocess(X_train)\n",
    "X_val, _ = preprocess(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "# import zipfile\n",
    "\n",
    "# # Define the URL for the GloVe embeddings\n",
    "# glove_url = \"http://nlp.stanford.edu/data/glove.twitter.27B.zip\"\n",
    "# glove_zip_file = \"glove.twitter.27B.zip\"\n",
    "\n",
    "# # Download the embeddings\n",
    "# response = requests.get(glove_url, stream=True)\n",
    "# with open(glove_zip_file, \"wb\") as file:\n",
    "#     for chunk in response.iter_content(chunk_size=128):\n",
    "#         file.write(chunk)\n",
    "\n",
    "# # Extract the embeddings\n",
    "# with zipfile.ZipFile(glove_zip_file, \"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"glove_embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_index = {}\n",
    "with open('glove_embeddings/glove.twitter.27B.200d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "# Create embedding matrix\n",
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(Y_train), y=Y_train.astype(int))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "#If class weight is used, y needs to be one-hot encoded\n",
    "Y_train_encoded = tf.keras.utils.to_categorical(Y_train, num_classes=3)\n",
    "Y_val_encoded = tf.keras.utils.to_categorical(Y_val, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models: Simple_RNN, LSTM and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSTM(vocab_size, input_length=MAX_SEQ_LEN):\n",
    "    text_input = Input(shape=(input_length,), name='text_input')\n",
    "    x = Embedding(input_dim=vocab_size+1, output_dim=128, input_length=input_length)(text_input)\n",
    "    x = LSTM(500, return_sequences=True)(x)\n",
    "    x = dropout(0.8)(x)\n",
    "    x = LSTM(300)(x)\n",
    "    x = dropout(0.5)(x)\n",
    "    output = Dense(3, activation='softmax')(x)\n",
    "    model = Model(inputs=[text_input], outputs=output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_RNN(vocab_size, input_length=MAX_SEQ_LEN):\n",
    "    text_input = Input(shape=(input_length,), name='text_input')\n",
    "    x = Embedding(input_dim=vocab_size+1, output_dim=128, input_length=MAX_SEQ_LEN)(text_input)\n",
    "    x = simpleRNN(500)(x)\n",
    "    x = dropout(0.8)(x)\n",
    "    #add_input = Input(shape=(4,), name='additional_input')\n",
    "    #x = Concatenate()([x, add_input])\n",
    "    #x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(3, activation='softmax')(x)\n",
    "    model = Model(inputs=[text_input], outputs=output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_GRU(vocab_size, input_length=MAX_SEQ_LEN):\n",
    "    learning_rate = 0.00001\n",
    "    text_input = Input(shape=(input_length,), name='text_input')\n",
    "    x = Embedding(input_dim=vocab_size+1, output_dim=128, input_length=MAX_SEQ_LEN)(text_input)\n",
    "    x = GRU(500)(x)\n",
    "    x = dropout(0.8)(x)\n",
    "    #add_input = Input(shape=(4,), name='additional_input')\n",
    "    #x = Concatenate()([x, add_input])\n",
    "    #x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(3, activation='softmax')(x)\n",
    "    model = Model(inputs=[text_input], outputs=output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def novel_model(vocab_size, input_length=MAX_SEQ_LEN, learning_rate=0.00003):\n",
    "    Bidirectional = tf.keras.layers.Bidirectional\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, \n",
    "                        output_dim=200, \n",
    "                        weights=[embedding_matrix], \n",
    "                        input_length=input_length, \n",
    "                        trainable=False))\n",
    "    model.add(Bidirectional(GRU(40, activation='relu', return_sequences=True, name='BiDirectionalGRUlayer'))) #GRU layer with 40 units\n",
    "    model.add(dropout(0.4)) #Dropout layer to prevent overfitting\n",
    "    model.add(Bidirectional(LSTM(20, recurrent_activation='relu', return_sequences=True))) #LSTM layer with 20 units\n",
    "    model.add(dropout(0.4)) #Dropout layer to prevent overfitting\n",
    "    model.add(Dense(15)) #Dense layer with 16 units and relu activation function\n",
    "    model.add(dropout(0.4)) #Dropout layer to prevent overfitting\n",
    "    model.add(simpleRNN(10)) #SimpleRNN layer with 10 units\n",
    "    model.add(dropout(0.4)) #Dropout layer to prevent overfitting\n",
    "    model.add(Dense(10)) #Dense layer with 16 units and relu activation function\n",
    "    model.add(Dense(3, activation='softmax')) #Output layer with 3 units and softmax activation function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, use_ema=True, ema_momentum=0.9)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, Y_train, X_val, Y_val, epochs=100, batch_size=64):\n",
    "      callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=6,\n",
    "                                                      restore_best_weights=True)\n",
    "      history = model.fit(X_train, Y_train, \n",
    "            validation_data=(X_val, Y_val), \n",
    "            epochs=epochs, batch_size=batch_size, \n",
    "            class_weight=class_weights,\n",
    "            callbacks=callback,\n",
    "            verbose=2)\n",
    "      print(\"\\n\\n****************************\\n\\n\")\n",
    "      print(\"Model trained successfully\")\n",
    "      pred = model.predict(X_val, batch_size=batch_size, verbose=1, steps=None)\n",
    "      y_true = np.argmax(Y_val, axis=1)\n",
    "      y_pred = np.argmax(pred, axis=1)\n",
    "      precision = precision_score(y_true, y_pred, average='weighted')\n",
    "      recall = recall_score(y_true, y_pred, average='weighted')\n",
    "      f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "      # Calculate AUC and plot ROC curve\n",
    "      if Y_val.shape[1] == 2:  # Binary classification case\n",
    "        auc = roc_auc_score(Y_val[:, 1], pred[:, 1])\n",
    "        fpr, tpr, _ = roc_curve(Y_val[:, 1], pred[:, 1])\n",
    "      else:  # Multi-class case (one-vs-rest approach)\n",
    "        auc = roc_auc_score(Y_val, pred, multi_class='ovr', average='weighted')\n",
    "        fpr, tpr, _ = roc_curve(Y_val.ravel(), pred.ravel())\n",
    "    \n",
    "      print(\"AUC: \", auc)\n",
    "      print(\"Precision: \", precision, \"Recall: \", recall, \"F1 Score: \", f1)\n",
    "      print(\"Validation Accuracy: \", model.evaluate(X_val, Y_val, batch_size=batch_size, verbose=1))\n",
    "      \n",
    "      # Plot ROC curve\n",
    "      plt.figure(figsize=(8, 6))\n",
    "      plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc:.2f})')\n",
    "      plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "      plt.xlim([0.0, 1.0])\n",
    "      plt.ylim([0.0, 1.05])\n",
    "      plt.xlabel('False Positive Rate')\n",
    "      plt.ylabel('True Positive Rate')\n",
    "      plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "      plt.legend(loc='lower right')\n",
    "      plt.show()\n",
    "      \n",
    "      return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(history):\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(0, len(train_accuracy)), train_accuracy, 'b-', linewidth=2, label='Training Accuracy')\n",
    "    plt.plot(range(0, len(val_accuracy)), val_accuracy, 'r-', linewidth=2, label='Validation Accuracy')\n",
    "    plt.title('Training & validation accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(0, len(train_loss)), train_loss, 'b-', linewidth=2, label='Training Loss')\n",
    "    plt.plot(range(0, len(val_loss)), val_loss, 'r-', linewidth=2, label='Validation Loss')\n",
    "    plt.title('Training & validation loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of BaseLine Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model_LSTM(vocab_size)\n",
    "history_LSTM = train(m, X_train, Y_train_encoded, X_val, Y_val_encoded, epochs=100, batch_size=512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(history_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model_RNN(vocab_size, input_length=MAX_SEQ_LEN)\n",
    "history_RNN = train(m, X_train, Y_train_encoded, X_val, Y_val_encoded, epochs=100, batch_size=512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(history_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model_GRU(vocab_size, input_length=MAX_SEQ_LEN)\n",
    "history_GRU = train(m, X_train, Y_train_encoded, X_val, Y_val_encoded, epochs=100, batch_size=256) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(history_GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Novel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = novel_model(vocab_size, input_length=MAX_SEQ_LEN, learning_rate=0.00003)\n",
    "history = train(m, X_train, Y_train_encoded, X_val, Y_val_encoded, epochs=100, batch_size=512)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
